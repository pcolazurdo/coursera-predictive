{"name":"Coursera-predictive","tagline":"Coursera Practical Machine Learning Course Project Submission","body":"---\r\ntitle: \"Practical Machine Learning Course Project\"\r\nauthor: \"pcolazurdo@yahoo.com\"\r\ndate: \"Sunday, March 22, 2015\"\r\noutput: html_document\r\n---\r\n\r\n\r\n```{r}\r\n# Load needed libraries\r\nlibrary(caret)\r\nlibrary(rpart)\r\nlibrary(randomForest)\r\n\r\n#Remove DIV/0! from the source file\r\nxx <- read.table(\"/Users/IBM_ADMIN/Dropbox/Personal/Courses/2015/Predictive/Course Project/pml-training.csv\", header=TRUE, sep=\",\", quote=\"\\\"\", na.strings = \"NA\", stringsAsFactors = FALSE)\r\nyy <- read.table(\"/Users/IBM_ADMIN/Dropbox/Personal/Courses/2015/Predictive/Course Project/pml-testing.csv\", header=TRUE, sep=\",\", quote=\"\\\"\", na.strings = \"NA\", stringsAsFactors = FALSE)\r\n\r\n#Set to 0 every NA value\r\nxx[is.na(xx)] = 0.00 \r\nyy[is.na(yy)] = 0.00 \r\n# remove text columns\r\ntraining <- xx[,-c(6,5,2)]\r\ntesting <- yy[,-c(6,5,2)]\r\n\r\n\r\n#remove empty columns\r\ndrops <- c(\"kurtosis_yaw_belt\", \"skewness_yaw_belt\", \"amplitude_yaw_belt\", \"kurtosis_yaw_dumbbell\", \"skewness_yaw_dumbbell\", \"amplitude_yaw_dumbbell\", \"kurtosis_yaw_forearm\", \"skewness_yaw_forearm\", \"amplitude_yaw_forearm\") \r\nfit <- training[,! names(training) %in% drops]\r\nfit <- fit[-c(1,2,3,4)]\r\nfit$classe <- as.factor(fit$classe)\r\n\r\ntest <- testing[,! names(testing) %in% drops]\r\ntest <- test[-c(1,2,3,4)]\r\n#test$classe <- as.factor(test$classe)\r\n\r\n\r\n# This was used to create a prediction model and verify it against a testing set ... I've used several differente models until I've found a table with very low error.\r\n\r\n\r\n#First Try\r\n#Run RPART to create Tree Model\r\nrp <- rpart(classe ~ ., fitTraining)\r\npred <- predict(rp,fitTesting, type=\"class\")\r\ntable(pred, fitTesting[,\"classe\"])\r\n\r\n# Manually choose most representative variables on the decision tree to create a new dataset\r\ntrainPlot1 <- fitTraining[,names(fitTraining) %in% c(\"roll_belt\",\"pitch_forearm\",\"roll_forearm\",\"yaw_belt\",\"pitch_belt\", \"yaw_forearm\" ,\"classe\")]\r\ntestPlot1 <- fitTesting[,names(fitTraining) %in% c(\"roll_belt\",\"pitch_forearm\",\"roll_forearm\",\"yaw_belt\",\"pitch_belt\", \"yaw_forearm\" ,\"classe\")]\r\n\r\n\r\n\r\n#Run a new decision tree with these new vars\r\nrp <- rpart(classe ~ ., data=trainPlot1)\r\npred <- predict(rp, testPlot1, type=\"class\")\r\ntable(pred, testPlot1[,\"classe\"])\r\n\r\n\r\n#Another Try\r\nrf <- randomForest(classe ~ ., data=trainPlot1, ntree=20, mtry=5, importance=TRUE)\r\ntestPlot1$pred <- predict(rf,testPlot1, type=\"response\")\r\ntable(testPlot1$pred, testPlot1$classe)\r\n\r\n\r\n#Best Try\r\ninTrain <- createDataPartition(y=fit$classe, p=0.85, list=FALSE)\r\nfitTraining = fit[inTrain,]\r\nfitTesting = fit[-inTrain,]\r\nrf <- randomForest(classe ~ ., data=fitTraining, ntree=100, mtry=15, importance=TRUE)\r\nfitTesting$pred <- predict(rf,fitTesting, type=\"response\")\r\ntable(fitTesting$pred, fitTesting$classe)\r\n\r\n\r\n#So this work fines ---\r\n#Retrain the algorithm with the 100% of training cases and run the algorithm against the testing set\r\ninTrain <- createDataPartition(y=fit$classe, p=1, list=FALSE)\r\nfitTraining = fit[inTrain,]\r\nrf <- randomForest(classe ~ ., data=fitTraining, ntree=100, mtry=15, importance=TRUE)\r\ntest$pred <- predict(rf,test, type=\"response\")\r\n\r\npml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(\"c:\\\\temp\\\\problem_id_\",i,\".txt\")\r\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\n\r\npml_write_files(test$pred)\r\n```\r\n\r\nNo Plots embedded\r\n\r\n```{r, echo=FALSE}\r\n\r\n```\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}